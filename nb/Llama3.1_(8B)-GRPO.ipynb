{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SALMANKHANPM/AutoGPT/blob/master/nb/Llama3.1_(8B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5nI_VnC8n97"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4FyKS_p8n98"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbX7gY548n9-"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-r5n1yb8n9-"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU0aa4PZ8n9_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfuWo5k48n9_"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# System Prompt Template\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# XML Format Template\n",
        "XML_COT_FORMAT = \"\"\"\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Output Format\n",
        "OUTPUT_FORMAT = {\n",
        "    \"Detected Topics\": \"\",\n",
        "    \"Confidence\": \"\",\n",
        "    \"Guardrail Context (10 words max)\": \"\",\n",
        "    \"Censored\": False  # or True\n",
        "}\n",
        "\n",
        "# Answer Extraction Functions\n",
        "def extract_xml_answer(text):\n",
        "    match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "def extract_hash_answer(text):\n",
        "    parts = text.split(\"####\")\n",
        "    return parts[1].strip() if len(parts) > 1 else None\n",
        "\n",
        "# Load JSON Dataset with Debugging\n",
        "def load_json_dataset(file_path=\"outputv2.json\"):\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File '{file_path}' not found!\")\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Check if JSON is a list of dicts\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"Expected a list of dictionaries in JSON file.\")\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Print column names for verification\n",
        "        print(\"Columns in dataset:\", df.columns)\n",
        "\n",
        "        # Expected columns\n",
        "        required_columns = [\"prompt_en\", \"prompt_te\", \"category\", \"subcategory\", \"intent\", \"potential_risk\"]\n",
        "\n",
        "        # Check for missing columns\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            raise KeyError(f\"Missing columns: {missing_columns}\")\n",
        "\n",
        "        # Check for missing values\n",
        "        if df[required_columns].isnull().sum().sum() > 0:\n",
        "            print(\"Warning: Missing values found in dataset!\")\n",
        "            print(df[required_columns].isnull().sum())\n",
        "\n",
        "        # Ensure valid index\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "        # Process rows and check for issues\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                df.at[idx, \"prompt\"] = [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": row[\"prompt_en\"]}\n",
        "                ]\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row {idx}: {e}\")\n",
        "                print(row)\n",
        "                continue  # Skip problematic row\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_json_dataset()\n",
        "if dataset.empty:\n",
        "    print(\"Dataset failed to load. Check for issues.\")\n",
        "\n",
        "# Reward Functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [{\n",
        "        **OUTPUT_FORMAT,\n",
        "        \"Confidence\": 2.0 if r == a else 0.0,\n",
        "        \"Detected Topics\": \"Correctness\",\n",
        "    } for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs):\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [{\n",
        "        **OUTPUT_FORMAT,\n",
        "        \"Confidence\": 0.5 if r.isdigit() else 0.0,\n",
        "        \"Detected Topics\": \"Integer Response\",\n",
        "    } for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs):\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [{\n",
        "        **OUTPUT_FORMAT,\n",
        "        \"Confidence\": 0.5 if re.fullmatch(pattern, r, re.DOTALL) else 0.0,\n",
        "        \"Detected Topics\": \"Strict Format Compliance\",\n",
        "    } for r in responses]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs):\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [{\n",
        "        **OUTPUT_FORMAT,\n",
        "        \"Confidence\": 0.5 if re.search(pattern, r, re.DOTALL) else 0.0,\n",
        "        \"Detected Topics\": \"Soft Format Compliance\",\n",
        "    } for r in responses]\n",
        "\n",
        "def count_xml(text):\n",
        "    score = sum([\n",
        "        0.125 if text.count(\"<reasoning>\\n\") == 1 else 0.0,\n",
        "        0.125 if text.count(\"\\n</reasoning>\\n\") == 1 else 0.0,\n",
        "        0.125 if text.count(\"\\n<answer>\\n\") == 1 else 0.0,\n",
        "        0.125 if text.count(\"\\n</answer>\") == 1 else 0.0\n",
        "    ])\n",
        "    penalty = len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001 if \"\\n</answer>\\n\" in text else 0.0\n",
        "    return score - penalty\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs):\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [{\n",
        "        **OUTPUT_FORMAT,\n",
        "        \"Confidence\": count_xml(c),\n",
        "        \"Detected Topics\": \"XML Structure Quality\",\n",
        "    } for c in contents]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = 256\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "qtcz_lpbVC92",
        "outputId": "70e4f329-acac-4d31-a8cd-47f7c6088747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.81s/it, est. speed input: 0.75 toks/s, output: 17.01 toks/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Calculating Pi using Python**\\n\\nPi (π) is a mathematical constant representing the ratio of a circle\\'s circumference to its diameter. Here\\'s a simple and efficient way to calculate an approximation of pi using Python.\\n\\n### Using the Monte Carlo Method\\n\\nThe Monte Carlo method is a computational algorithm that uses random sampling to approximate a value. In this case, we can use it to estimate pi by generating random points within a square and checking if they fall inside a quarter-circle inscribed within it.\\n\\n```python\\nimport random\\nimport math\\n\\ndef estimate_pi(num_samples):\\n    \"\"\"\\n    Estimate the value of pi using the Monte Carlo method.\\n\\n    Args:\\n    num_samples (int): The number of random points to generate.\\n\\n    Returns:\\n    float: An approximation of pi.\\n    \"\"\"\\n    points_inside_circle = 0\\n\\n    for _ in range(num_samples):\\n        x, y = random.random(), random.random()\\n        distance = x**2 + y**2\\n        if distance <= 1:\\n            points_inside_circle += 1\\n\\n    return (points_inside_circle / num_samples) * 4\\n\\n# Example usage:\\nnum_samples = 1000000\\napprox_pi = estimate_pi(num_samples)\\nprint(f\"Approximation of pi with {num_samples} samples: {approx_pi}\")\\n```\\n\\nThis code generates `num_samples` random points within the square (-1, -1) to (1, 1) and checks if each point falls inside the quarter-circle. The ratio of points inside the circle to the total number of samples multiplied by 4 gives an approximation of pi.\\n\\n### Using the Bailey-Borwein-Plouffe Formula\\n\\nThe Bailey-Borwein-Plouffe (BBP) formula is another way to calculate pi. It\\'s a spigot algorithm that uses a series expansion to generate the digits of pi.\\n\\n```python\\ndef bbp_pi(n):\\n    \"\"\"\\n    Calculate the Bailey-Borwein-Plouffe formula for pi.\\n\\n    Args:\\n    n (int): The number of terms to use in the series expansion.\\n\\n    Returns:\\n    float: An approximation of pi.\\n    \"\"\"\\n    pi = 0.0\\n    for k in range(n):\\n        pi += (1 / (16 ** k)) * (\\n            4 / (8 * k + 1) -\\n            2 / (8 * k + 4) -\\n            1 / (8 * k + 5) -\\n            1 / (8 * k + 6)\\n        )\\n    return pi\\n\\n# Example usage:\\nn = 100\\napprox_pi = bbp_pi(n)\\nprint(f\"Approximation of pi with {n} terms: {approx_pi}\")\\n```\\n\\nThis code calculates an approximation of pi using the BBP formula with `n` terms.\\n\\n### Using the Gauss-Legendre Algorithm\\n\\nThe Gauss-Legendre algorithm is another method for calculating pi. It\\'s a recursive algorithm that uses an iterative process to improve the estimate of pi.\\n\\n```python\\ndef gauss_legendre(n):\\n    \"\"\"\\n    Calculate pi using the Gauss-Legendre algorithm.\\n\\n    Args:\\n    n (int): The number of iterations.\\n\\n    Returns:\\n    float: An approximation of pi.\\n    \"\"\"\\n    a, b = 1.0, (1 + math.sqrt(5)) / 2.0\\n    t = (1 + math.sqrt(5)) / 4.0\\n    p = 1.0\\n    for _ in range(n):\\n        a, b = (a + b) / 2.0, math.sqrt(a * b)\\n        t -= p * (a - b)\\n        p *= 2.0\\n        a = a + t / p\\n        b = math.sqrt(b - t / p)\\n    return a * b * 16.0\\n\\n# Example usage:\\nn = 100\\napprox_pi = gauss_legendre(n)\\nprint(f\"Approximation of pi with {n} iterations: {approx_pi}\")\\n```\\n\\nThis code calculates an approximation of pi using the Gauss-Legendre algorithm with `n` iterations.\\n\\nRemember that these are simplified examples and may not be the most efficient or accurate methods for calculating pi. For more accurate calculations, you can use specialized libraries or software.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "zf_OY5WMVOxF",
        "outputId": "22373f16-a3bc-4c99-8a1d-1994522a5f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.17s/it, est. speed input: 2.63 toks/s, output: 15.80 toks/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Calculating pi to a high degree of accuracy is a complex task that requires a large amount of computational power. However, I can provide you with an approximate value of pi or show you a simple method to calculate it.\\n\\nOne of the simplest methods to calculate pi is the Leibniz formula, which is an infinite series:\\n\\npi/4 = 1 - 1/3 + 1/5 - 1/7 + 1/9 - ...\\n\\nThis series can be used to calculate an approximation of pi.\\n\\n<reasoning>\\npi = 4 * (1 - 1/3 + 1/5 - 1/7 + 1/9 - ...)\\n</reasoning>\\n\\nThis is a simple, yet effective method to calculate pi. However, the more terms you use, the more accurate the result will be.\\n\\nTo calculate pi to a high degree of accuracy, you would need to use a computer program to perform the calculation.\\n\\n<answer>\\n3.141592653589793 (approximately)\\n</answer>\\n\\nFor a more accurate result, I can provide you with a Python code snippet to calculate pi:\\n\\n```python\\nimport math\\n\\ndef calculate_pi(n):\\n    pi = 0.0\\n    for i in range(n):\\n        pi += ((-1)**i) / (2 * i + 1)\\n    return 4 * pi\\n\\nn = 1000000  # Increase the value of n for more accurate result\\nprint(calculate_pi(n))\\n```\\n\\nThis code uses the Leibniz formula to calculate pi to a high degree of accuracy. The value of `n` can be increased for a more accurate result.\\n\\nNote that this is just one of many methods to calculate pi, and there are more complex algorithms available for more accurate results.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMYggpEb8n-G"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}